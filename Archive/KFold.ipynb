{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c014a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MNIST trainloaders\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common import _get_model, create_worker_trainloaders, LOSS_FUNC\n",
    "\n",
    "DEFAULT_K_SPLITS = 5\n",
    "\n",
    "\"\"\"parser = argparse.ArgumentParser(description='KFold Cross Validation ')\n",
    "parser.add_argument('--dataset', type=str, default=\"mnist\", choices=[\"mnist\", \"fashion_mnist\", \"cifar10\", \"cifar100\"],\n",
    "                    help=\"Dataset name for KFold CV: mnist, FashionMNIST, CIFAR10, CIFAR100\")\n",
    "parser.add_argument('--k_splits', type=int, default=DEFAULT_K_SPLITS, help=\"The number of splits for KFold CV.\")\n",
    "args = parser.parse_args()\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"if args.k_splits < 2:\n",
    "    print(\"Forbiden value!!! --k_split should be >= 2\")\n",
    "    exit()\"\"\"\n",
    "\n",
    "loader = create_worker_trainloaders(\"mnist\", 1, batch_size=100, model_accuracy=False)[0]\n",
    "indices = []\n",
    "for batch_indices, _ in loader:\n",
    "    indices.extend(batch_indices.numpy())\n",
    "indices = np.array(indices)\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "\n",
    "\"\"\"learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "momentums = [0.9, 0.95, 0.99]\n",
    "batch_sizes = [32, 64, 100, 128]\n",
    "epochs = [2, 4, 6, 8, 10]\"\"\"\n",
    "learning_rates = [0.001, 0.1]\n",
    "momentums = [0.9]\n",
    "batch_sizes = [128]\n",
    "epochs = [1, 2]\n",
    "\n",
    "total_steps = len(epochs) * len(learning_rates) * len(momentums) * len(batch_sizes)\n",
    "current_step = 0\n",
    "avg_losses = np.zeros(\n",
    "    (len(epochs), len(learning_rates), len(momentums), len(batch_sizes))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665e85ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1/4, Fold: 1/2\n",
      "Created MNIST CNN\n",
      "Step: 1/4, Fold: 2/2\n",
      "Created MNIST CNN\n",
      "Step: 2/4, Fold: 1/2\n",
      "Created MNIST CNN\n",
      "Step: 2/4, Fold: 2/2\n",
      "Created MNIST CNN\n",
      "Step: 3/4, Fold: 1/2\n",
      "Created MNIST CNN\n",
      "Step: 3/4, Fold: 2/2\n",
      "Created MNIST CNN\n",
      "Step: 4/4, Fold: 1/2\n",
      "Created MNIST CNN\n",
      "Step: 4/4, Fold: 2/2\n",
      "Created MNIST CNN\n",
      "Best parameters: epochs=2, learning_rate=0.1, momentum=0.9, batch_size=128, loss=0.17911546757405108\n",
      " epochs  learning_rate  momentum  batch_size  Average Loss\n",
      "      1          0.001       0.9         128      1.943376\n",
      "      1          0.100       0.9         128      1.360757\n",
      "      2          0.001       0.9         128      0.601652\n",
      "      2          0.100       0.9         128      0.179115\n"
     ]
    }
   ],
   "source": [
    "for epoch_index, epoch in enumerate(epochs):\n",
    "    for lr_index, learning_rate in enumerate(learning_rates):\n",
    "        for momentum_index, momentum in enumerate(momentums):\n",
    "            for batch_size_index, batch_size in enumerate(batch_sizes):\n",
    "                avg_loss = 0.0\n",
    "                for fold, (train_indices, val_indices) in enumerate(kf.split(indices)):\n",
    "                    print(\n",
    "                        f\"Step: {current_step+1}/{total_steps}, Fold: {fold + 1}/{kf.get_n_splits()}\"\n",
    "                    )\n",
    "                    model = _get_model(\"mnist\", LOSS_FUNC)\n",
    "                    optimizer = optim.SGD(\n",
    "                        model.parameters(), lr=learning_rate, momentum=momentum\n",
    "                    )\n",
    "                    train_sampler = SubsetRandomSampler(train_indices)\n",
    "                    val_sampler = SubsetRandomSampler(val_indices)\n",
    "                    train_dataloader = DataLoader(\n",
    "                        loader.dataset, batch_size=batch_size, sampler=train_sampler\n",
    "                    )\n",
    "                    val_dataloader = DataLoader(\n",
    "                        loader.dataset, batch_size=batch_size, sampler=val_sampler\n",
    "                    )\n",
    "\n",
    "                    for _ in range(epoch):\n",
    "                        for data, target in train_dataloader:\n",
    "                            optimizer.zero_grad()\n",
    "                            output = model(data)\n",
    "                            loss = LOSS_FUNC(output, target)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    val_loss = 0.0\n",
    "                    val_count = 0\n",
    "                    with torch.no_grad():\n",
    "                        for data, target in val_dataloader:\n",
    "                            output = model(data)\n",
    "                            loss = LOSS_FUNC(output, target)\n",
    "                            val_loss += loss.item()\n",
    "                            val_count += 1\n",
    "                    avg_loss += val_loss / val_count\n",
    "\n",
    "                avg_loss /= kf.get_n_splits()\n",
    "                avg_losses[\n",
    "                    epoch_index, lr_index, momentum_index, batch_size_index\n",
    "                ] = avg_loss\n",
    "                current_step += 1\n",
    "\n",
    "min_loss_index = np.unravel_index(np.argmin(avg_losses, axis=None), avg_losses.shape)\n",
    "min_loss_value = avg_losses[min_loss_index]\n",
    "print(\n",
    "    f\"\\nBest parameters: epochs={epochs[min_loss_index[0]]}, learning_rate={learning_rates[min_loss_index[1]]}, momentum={momentums[min_loss_index[2]]}, batch_size={batch_sizes[min_loss_index[3]]}, loss={min_loss_value}\"\n",
    ")\n",
    "\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [epochs, learning_rates, momentums, batch_sizes],\n",
    "    names=[\"epochs\", \"learning_rate\", \"momentum\", \"batch_size\"],\n",
    ")\n",
    "df = pd.DataFrame(avg_losses.flatten(), index=index, columns=[\"Average Loss\"])\n",
    "print(df.reset_index().to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
