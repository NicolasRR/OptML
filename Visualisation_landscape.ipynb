{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "METHOD\n",
    "\n",
    "1) Train a model fully with the leNet5 net and the mnist dataset\n",
    "2) Retrive it's weights (for each parameters and compute it's loss function)\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the LeNet5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Train the model and store the loss values\n",
    "model = LeNet5()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(5):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_values.append(loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways of plotting th loss function \n",
    "Neural networks are trained on a corpus of feature vectors (e.g., images) {xi} and accompanying\n",
    "labels {yi} by minimizing a loss of the form L(θ) = sum (for i in dataset) Loss (xi, yi; θ), where θ denotes the\n",
    "parameters (weights) of the neural network, the function `(xi, yi; θ) measures how well the neural\n",
    "network with parameters θ predicts the label of a data sample, and m is the number of data samples.\n",
    "Neural nets contain many parameters, and so their loss functions live in a very high-dimensional\n",
    "space. Unfortunately, visualizations are only possible using low-dimensional 1D (line) or 2D (surface)\n",
    "plots. Several methods exist for closing this dimensionality gap.\n",
    "1-Dimensional Linear Interpolation One simple and lightweight way to plot loss functions is\n",
    "to choose two sets of parameters θ and θ\n",
    "0\n",
    ", and plot the values of the loss function along the line\n",
    "connecting these two points. We can parameterize this line by choosing a scalar parameter α, and\n",
    "defining the weighted average θ(α) = (1−α)θ+αθ0\n",
    ". Finally, we plot the function f(α) = L(θ(α)).\n",
    "This strategy was taken by Goodfellow et al. [13], who studied the loss surface along the line between\n",
    "a random initial guess, and a nearby minimizer obtained by stochastic gradient descent. This method\n",
    "has been widely used to study the “sharpness” and “flatness” of different minima, and the dependence. However there is some limitation in the plotting of such a 1D loss landscape.\n",
    "\n",
    "For 2D visulization : \n",
    "Contour Plots & Random Directions To use this approach, one chooses a center point θ in the graph, and chooses two direction vectors, δ and η. One then plots a function of the form\n",
    "f(α) = L(θ + αδ) in the 1D (line) case, or f(α, β) = L(θ∗ + αδ + βη)\n",
    "\n",
    "# Interesting approch : Filter-Wise Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000279E7049070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 150",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39mparameters()))))\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, params \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mparameters()):\n\u001b[1;32m----> 4\u001b[0m     weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((weights, params\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mflatten()\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m X, Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmeshgrid(\u001b[39mrange\u001b[39m(weights\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]), \u001b[39mrange\u001b[39m(weights\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      7\u001b[0m \u001b[39m# Evaluate the loss function at each point in the meshgrid\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 1 has size 150"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a meshgrid of weight values\n",
    "weights = np.empty((0, len(list(model.parameters()))))\n",
    "for i, params in enumerate(model.parameters()):\n",
    "    weights = np.concatenate((weights, params.data.flatten().numpy().reshape(1, -1)), axis=0)\n",
    "X, Y = np.meshgrid(range(weights.shape[1]), range(weights.shape[0]))\n",
    "\n",
    "# Evaluate the loss function at each point in the meshgrid\n",
    "Z = []\n",
    "for row in range(weights.shape[0]):\n",
    "    params = torch.from_numpy(weights[row]).float().view_as(params)\n",
    "    for i, params_i in enumerate(model.parameters()):\n",
    "        params_i.data = params[i].view_as(params_i.data)\n",
    "    output = model(images)\n",
    "    loss = criterion(output, labels)\n",
    "    Z.append(loss.item())\n",
    "Z = np.array(Z).reshape(X.shape)\n",
    "\n",
    "# Create a contour plot of the loss function landscape\n",
    "plt.contourf(X, Y, Z, levels=50)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
